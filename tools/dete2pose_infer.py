import cv2
import numpy as np

from mmdet.apis import init_detector, inference_detector
from mmpose.apis import init_model as init_pose_model
from mmpose.apis import inference_topdown
from mmpose.structures import merge_data_samples


# =========================
# 1. è·¯å¾„é…ç½®ï¼ˆä½ åªéœ€è¦æ”¹è¿™é‡Œï¼‰
# =========================
DET_CONFIG = '/home/tbai/mmdetection/mmdetection/rtmdet_human_myAAG.py'
DET_CKPT   = 'work_dirs/rtmdet_human_myAAG/best_coco_bbox_mAP_epoch_30.pth'

POSE_CONFIG = '/home/tbai/mmpose/timm_mobilevit_pipeline.py'
POSE_CKPT   = '/home/tbai/mmpose/work_dirs/mobilevit_pose_ep10/epoch_40.pth'

IMG_PATH = '/mnt/dst_datasets3/project/SensIR/SensIR/SenIRDatasetProcessed/1A-2 Walking/AAG/undistorted_stitched_png/frame_0030.png'
DEVICE = 'cuda:0'


# =========================
# 2. åˆå§‹åŒ–æ¨¡å‹
# =========================
det_model = init_detector(DET_CONFIG, DET_CKPT, device=DEVICE)
pose_model = init_pose_model(POSE_CONFIG, POSE_CKPT, device=DEVICE)


# =========================
# 3. Detector æ¨ç†
# =========================
det_result = inference_detector(det_model, IMG_PATH)

pred = det_result.pred_instances
bboxes = pred.bboxes.cpu().numpy()      # (N, 4) xyxy
scores = pred.scores.cpu().numpy()
labels = pred.labels.cpu().numpy()

# ğŸ‘‰ å•ç±» human detectorï¼šlabel == 0
keep = (scores > 0.3) & (labels == 0)
bboxes = bboxes[keep]
scores = scores[keep]

if len(bboxes) == 0:
    raise RuntimeError('No human detected')

# å– score æœ€é«˜çš„ä¸€ä¸ªäºº
idx = np.argmax(scores)
bbox = bboxes[idx].tolist()
bbox_score = float(scores[idx])

print('[Detector] bbox:', bbox, 'score:', bbox_score)


# =========================
# 4. ç”¨ detector bbox å–‚ç»™ MMPose
# =========================
person_results = [{
    'bbox': bbox,
    'bbox_score': bbox_score
}]

pose_results = inference_topdown(
    pose_model,
    IMG_PATH,
    person_results
)

pose_results = merge_data_samples(pose_results)


# =========================
# 5. å¯è§†åŒ–ï¼ˆbbox + keypointsï¼‰
# =========================
img = cv2.imread(IMG_PATH)
if img.ndim == 2:
    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)

pose_model.visualizer.set_image(img)
pose_model.visualizer.add_datasample(
    'result',
    img,
    pose_results,
    draw_gt=False,
    draw_bbox=True,
    draw_heatmap=False,
    kpt_thr=0.3
)

vis_img = pose_model.visualizer.get_image()
cv2.imwrite('det2pose_result.png', vis_img)

print('âœ… Saved det2pose_result.png')


# =========================
# 6. æ‰“å°å…³é”®ç‚¹ï¼ˆæ•°å€¼éªŒè¯ï¼‰
# =========================
kpts = pose_results.pred_instances.keypoints[0].cpu().numpy()
print('[Pose] keypoints:')
print(kpts)

